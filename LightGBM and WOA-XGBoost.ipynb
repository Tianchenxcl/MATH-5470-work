{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54884148-8cc8-4f9b-b021-32441e50fb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
      "Collecting numpy\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/10/a2/010b0e27ddeacab7839957d7a8f00e91206e0c2c47abbb5f35a2630e5387/numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m744.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/bf/c9/63f8d545568d9ab91476b1818b4741f521646cbdd151c6efebf40d6de6f7/pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m769.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/10/b7/4aa196155b4d846bd749cf82aa5a4c300cf55a8b5e0dfa5b722a63c0f8a0/matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m828.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting seaborn\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/83/11/00d3c3dfc25ad54e731d91449895a79e4bf2384dc3ac01809010ba88f6d5/seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/ef/0e/97dbca66347b8cf0ea8b529e6bb9367e337ba2e8be0ef5c1a545232abfde/scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m709.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting lightgbm\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/42/86/dabda8fbcb1b00bcfb0003c3776e8ade1aa7b413dff0a2c08f457dace22f/lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m862.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/5f/4b/6157f24ca425b89fe2eb7e7be642375711ab671135be21e6faa100f7448c/contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/d2/d2/9f4e4c4374dd1daa8367784e1bd910f18ba886db1d6b825b12edf6db3edc/fonttools-4.60.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m574.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/66/e1/e533435c0be77c3f64040d68d7a657771194a63c279f55573188161e81ca/kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m502.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.2.0)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/10/5e/1aa9a93198c6b64513c9d7752de7422c06402de6600a8767da1524f9570b/pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/1a/bc/a5c75095089b96ea72c1bd37a4497c24b581ec73db4ef58ebee142ad2d14/scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m631.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/1e/e8/685f47e0d754320684db4425a0967f7d3fa70126bffd76110b7009a0090f/joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/32/d5/f9a850d79b0851d1d4ef6456097579a9005b31fea68726a4ae5f2d82ddd9/threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, pyparsing, numpy, kiwisolver, joblib, fonttools, cycler, scipy, pandas, contourpy, scikit-learn, matplotlib, lightgbm, seaborn\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 2.4.7\n",
      "    Uninstalling pyparsing-2.4.7:\n",
      "      Successfully uninstalled pyparsing-2.4.7\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 joblib-1.5.2 kiwisolver-1.4.9 lightgbm-4.6.0 matplotlib-3.10.7 numpy-2.3.3 pandas-2.3.3 pyparsing-3.2.5 pytz-2025.2 scikit-learn-1.7.2 scipy-1.16.2 seaborn-0.13.2 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U numpy pandas matplotlib seaborn scikit-learn lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e277a1fc-56a9-4d97-9520-1e7881687b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.3.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
      "Collecting xgboost\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/64/ad/61a86228e981b15361ff963e84648b1a29ab43debd95f7c2b3ef9d94dca1/xgboost-3.0.5-py3-none-manylinux_2_28_x86_64.whl (94.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.20.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U numpy pandas matplotlib seaborn scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2352c4a-5325-4a2a-88a9-ad3b582d5421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost version: 3.0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:02:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:02:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC OK, GPU hist works.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"xgboost version:\", xgb.__version__)\n",
    "X, y = make_classification(n_samples=2000, n_features=30, random_state=0)\n",
    "Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    tree_method=\"gpu_hist\",   # 关键：GPU\n",
    "    predictor=\"gpu_predictor\",\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "clf.fit(Xtr, ytr, eval_set=[(Xva, yva)], verbose=False)\n",
    "print(\"AUC OK, GPU hist works.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32591ef-8fdd-4a1d-a2fe-94f088a4d06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading datasets...\n",
      "Number of training samples: 307511\n",
      "Number of test samples: 48744\n",
      "Read data - Completed in: 4 seconds\n",
      "Target variable distribution:\n",
      "TARGET\n",
      "0    282686\n",
      "1     24825\n",
      "Name: count, dtype: int64\n",
      "Positive sample ratio: 8.07%\n",
      "Number of categorical features: 16\n",
      "Data preprocessing - Completed in: 4 seconds\n",
      "Create basic features - Completed in: 0 seconds\n",
      "Process Bureau data - Completed in: 7 seconds\n",
      "Process Previous_application data - Completed in: 31 seconds\n",
      "Process Installments_payments data - Completed in: 9 seconds\n",
      "Process POS_CASH_balance data - Completed in: 7 seconds\n",
      "Process Credit_card_balance data - Completed in: 6 seconds\n",
      "Training set shape: (307511, 140)\n",
      "Test set shape: (48744, 139)\n",
      "Fill missing values - Completed in: 0 seconds\n",
      "Number of features: 138\n",
      "X shape: (307511, 138)\n",
      "y shape: (307511,)\n",
      "X_test shape: (48744, 138)\n",
      "WOA init best AUC: 0.776585\n",
      "WOA iter 1/15 best AUC: 0.777694\n",
      "WOA iter 2/15 best AUC: 0.777694\n",
      "WOA iter 3/15 best AUC: 0.777877\n",
      "WOA iter 4/15 best AUC: 0.777877\n",
      "WOA iter 5/15 best AUC: 0.777877\n",
      "WOA iter 6/15 best AUC: 0.777877\n",
      "WOA iter 7/15 best AUC: 0.777877\n",
      "WOA iter 8/15 best AUC: 0.777877\n",
      "WOA iter 9/15 best AUC: 0.777877\n",
      "WOA iter 10/15 best AUC: 0.777877\n",
      "WOA iter 11/15 best AUC: 0.777877\n",
      "WOA iter 12/15 best AUC: 0.777877\n",
      "WOA iter 13/15 best AUC: 0.777877\n",
      "WOA iter 14/15 best AUC: 0.777877\n",
      "WOA iter 15/15 best AUC: 0.777877\n",
      "Best params from WOA:\n",
      "  max_depth: 3\n",
      "  learning_rate: 0.08904763133147776\n",
      "  subsample: 0.907585088851951\n",
      "  colsample_bytree: 0.7461773201302205\n",
      "  reg_alpha: 7.357301275632957\n",
      "  reg_lambda: 0.0\n",
      "  min_child_weight: 24.540588435847106\n",
      "  gamma: 0.0\n",
      "  n_estimators (num_boost_round): 1666\n",
      "Fold 1\n",
      "[0]\ttrain-auc:0.65918\tvalid-auc:0.65559\n",
      "[200]\ttrain-auc:0.78111\tvalid-auc:0.76560\n",
      "[400]\ttrain-auc:0.79275\tvalid-auc:0.77140\n",
      "[600]\ttrain-auc:0.80115\tvalid-auc:0.77318\n",
      "[800]\ttrain-auc:0.80794\tvalid-auc:0.77391\n",
      "[1000]\ttrain-auc:0.81380\tvalid-auc:0.77419\n",
      "[1200]\ttrain-auc:0.81923\tvalid-auc:0.77408\n",
      "[1300]\ttrain-auc:0.82168\tvalid-auc:0.77393\n",
      "Fold 2\n",
      "[0]\ttrain-auc:0.65799\tvalid-auc:0.64988\n",
      "[200]\ttrain-auc:0.77921\tvalid-auc:0.77424\n",
      "[400]\ttrain-auc:0.79094\tvalid-auc:0.77988\n",
      "[600]\ttrain-auc:0.79905\tvalid-auc:0.78219\n",
      "[800]\ttrain-auc:0.80573\tvalid-auc:0.78348\n",
      "[1000]\ttrain-auc:0.81148\tvalid-auc:0.78403\n",
      "[1200]\ttrain-auc:0.81689\tvalid-auc:0.78402\n",
      "[1400]\ttrain-auc:0.82177\tvalid-auc:0.78411\n",
      "[1551]\ttrain-auc:0.82528\tvalid-auc:0.78387\n",
      "Fold 3\n",
      "[0]\ttrain-auc:0.65857\tvalid-auc:0.65736\n",
      "[200]\ttrain-auc:0.78107\tvalid-auc:0.76702\n",
      "[400]\ttrain-auc:0.79317\tvalid-auc:0.77305\n",
      "[600]\ttrain-auc:0.80126\tvalid-auc:0.77518\n",
      "[800]\ttrain-auc:0.80769\tvalid-auc:0.77590\n",
      "[1000]\ttrain-auc:0.81351\tvalid-auc:0.77615\n",
      "[1200]\ttrain-auc:0.81868\tvalid-auc:0.77592\n",
      "[1261]\ttrain-auc:0.82024\tvalid-auc:0.77605\n",
      "Fold 4\n",
      "[0]\ttrain-auc:0.65659\tvalid-auc:0.65514\n",
      "[200]\ttrain-auc:0.77943\tvalid-auc:0.77392\n",
      "[400]\ttrain-auc:0.79158\tvalid-auc:0.77890\n",
      "[600]\ttrain-auc:0.79977\tvalid-auc:0.78053\n",
      "[800]\ttrain-auc:0.80655\tvalid-auc:0.78138\n",
      "[1000]\ttrain-auc:0.81270\tvalid-auc:0.78176\n",
      "[1200]\ttrain-auc:0.81797\tvalid-auc:0.78204\n",
      "[1304]\ttrain-auc:0.82055\tvalid-auc:0.78208\n",
      "Fold 5\n",
      "[0]\ttrain-auc:0.65909\tvalid-auc:0.65382\n",
      "[200]\ttrain-auc:0.78117\tvalid-auc:0.76620\n",
      "[400]\ttrain-auc:0.79289\tvalid-auc:0.77160\n",
      "[600]\ttrain-auc:0.80096\tvalid-auc:0.77435\n",
      "[800]\ttrain-auc:0.80745\tvalid-auc:0.77539\n",
      "[1000]\ttrain-auc:0.81347\tvalid-auc:0.77577\n",
      "[1200]\ttrain-auc:0.81867\tvalid-auc:0.77583\n",
      "[1290]\ttrain-auc:0.82094\tvalid-auc:0.77585\n",
      "Full AUC score: 0.778587\n",
      "Train WOA-XGBoost (GPU) - Completed in: 1678 seconds\n",
      "Prediction results have been saved as submission.csv\n",
      "Generate prediction results - Completed in: 0 seconds\n",
      "Done! Final CV score: 0.7785867832288362\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Home Credit Default Risk - Runnable Script (WOA-XGBoost, GPU, xgb.train 兼容)\n",
    "# 数据目录：/hy-tmp/home-credit-default-risk/\n",
    "# 输出文件：lgbm_feature_importance.png, roc_curve.png, submission.csv\n",
    "# =========================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------- 基础设置 ----------\n",
    "DATA_DIR = \"/hy-tmp\"  # ← 你的数据目录\n",
    "os.makedirs(\".\", exist_ok=True)                # 确保当前目录可写\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f\"{title} - Completed in: {time.time() - t0:.0f} seconds\")\n",
    "\n",
    "def _try_paths(base_dir, name):\n",
    "    candidates = [f\"{name}.csv\", f\"{name}.CSV\", name]\n",
    "    for cand in candidates:\n",
    "        p = os.path.join(base_dir, cand)\n",
    "        if os.path.isfile(p):\n",
    "            return p\n",
    "    raise FileNotFoundError(\n",
    "        f\"Cannot find file for '{name}' in '{base_dir}'. Tried: {', '.join(candidates)}\"\n",
    "    )\n",
    "\n",
    "def read_csv_smart(base_dir, name, **kwargs):\n",
    "    path = _try_paths(base_dir, name)\n",
    "    return pd.read_csv(path, **kwargs)\n",
    "\n",
    "# ---------- 1. 读取数据 ----------\n",
    "print(\"Reading datasets...\")\n",
    "with timer(\"Read data\"):\n",
    "    app_train = read_csv_smart(DATA_DIR, \"application_train\")\n",
    "    app_test  = read_csv_smart(DATA_DIR, \"application_test\")\n",
    "    print('Number of training samples:', len(app_train))\n",
    "    print('Number of test samples:', len(app_test))\n",
    "\n",
    "# ---------- 2. 数据预处理 ----------\n",
    "with timer(\"Data preprocessing\"):\n",
    "    print('Target variable distribution:')\n",
    "    print(app_train['TARGET'].value_counts())\n",
    "    print('Positive sample ratio: {:.2%}'.format(app_train['TARGET'].mean()))\n",
    "    \n",
    "    for df in (app_train, app_test):\n",
    "        if 'DAYS_EMPLOYED' in df.columns:\n",
    "            df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    categorical_features = [col for col in app_train.columns if app_train[col].dtype == 'object']\n",
    "    print('Number of categorical features:', len(categorical_features))\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(app_train[col].astype(str).values) + list(app_test[col].astype(str).values))\n",
    "        app_train[col] = le.transform(list(app_train[col].astype(str).values))\n",
    "        app_test[col]  = le.transform(list(app_test[col].astype(str).values))\n",
    "\n",
    "# ---------- 3. 基础衍生特征 ----------\n",
    "with timer(\"Create basic features\"):\n",
    "    for df in (app_train, app_test):\n",
    "        df['CREDIT_INCOME_RATIO']  = df['AMT_CREDIT']  / df['AMT_INCOME_TOTAL']\n",
    "        df['ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "        df['CREDIT_TERM']          = df['AMT_CREDIT']  / df['AMT_ANNUITY']\n",
    "        df['DAYS_EMPLOYED_RATIO']  = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "\n",
    "# ---------- 4. Bureau ----------\n",
    "with timer(\"Process Bureau data\"):\n",
    "    bureau = read_csv_smart(DATA_DIR, \"bureau\")\n",
    "    bb     = read_csv_smart(DATA_DIR, \"bureau_balance\")  # 读取保持一致，不直接使用\n",
    "    \n",
    "    bureau_counts   = bureau.groupby('SK_ID_CURR')['SK_ID_BUREAU'].count().reset_index().rename(\n",
    "        columns={'SK_ID_BUREAU': 'BUREAU_LOAN_COUNT'})\n",
    "    bureau_avg_loan = bureau.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].mean().reset_index().rename(\n",
    "        columns={'AMT_CREDIT_SUM': 'BUREAU_AVG_LOAN'})\n",
    "    bureau_overdue  = bureau.groupby('SK_ID_CURR')['CREDIT_DAY_OVERDUE'].max().reset_index().rename(\n",
    "        columns={'CREDIT_DAY_OVERDUE': 'BUREAU_MAX_OVERDUE'})\n",
    "    \n",
    "    app_train = app_train.merge(bureau_counts,   on='SK_ID_CURR', how='left')\n",
    "    app_train = app_train.merge(bureau_avg_loan, on='SK_ID_CURR', how='left')\n",
    "    app_train = app_train.merge(bureau_overdue,  on='SK_ID_CURR', how='left')\n",
    "    \n",
    "    app_test  = app_test.merge(bureau_counts,   on='SK_ID_CURR', how='left')\n",
    "    app_test  = app_test.merge(bureau_avg_loan, on='SK_ID_CURR', how='left')\n",
    "    app_test  = app_test.merge(bureau_overdue,  on='SK_ID_CURR', how='left')\n",
    "\n",
    "    del bureau, bb, bureau_counts, bureau_avg_loan, bureau_overdue\n",
    "    gc.collect()\n",
    "\n",
    "# ---------- 5. Previous Application ----------\n",
    "with timer(\"Process Previous_application data\"):\n",
    "    prev = read_csv_smart(DATA_DIR, \"previous_application\")\n",
    "    \n",
    "    prev_app_counts = prev.groupby('SK_ID_CURR')['SK_ID_PREV'].count().reset_index().rename(\n",
    "        columns={'SK_ID_PREV': 'PREV_APP_COUNT'})\n",
    "    prev_app_amt = prev.groupby('SK_ID_CURR')['AMT_CREDIT'].mean().reset_index().rename(\n",
    "        columns={'AMT_CREDIT': 'PREV_APP_AVG_AMOUNT'})\n",
    "    prev_app_rejected = prev.groupby('SK_ID_CURR')['NAME_CONTRACT_STATUS'].apply(\n",
    "        lambda x: (x == 'Refused').sum() / len(x)\n",
    "    ).reset_index().rename(columns={'NAME_CONTRACT_STATUS': 'PREV_APP_REJECTION_RATIO'})\n",
    "    \n",
    "    app_train = app_train.merge(prev_app_counts,   on='SK_ID_CURR', how='left')\n",
    "    app_train = app_train.merge(prev_app_amt,      on='SK_ID_CURR', how='left')\n",
    "    app_train = app_train.merge(prev_app_rejected, on='SK_ID_CURR', how='left')\n",
    "    \n",
    "    app_test  = app_test.merge(prev_app_counts,   on='SK_ID_CURR', how='left')\n",
    "    app_test  = app_test.merge(prev_app_amt,      on='SK_ID_CURR', how='left')\n",
    "    app_test  = app_test.merge(prev_app_rejected, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    del prev, prev_app_counts, prev_app_amt, prev_app_rejected\n",
    "    gc.collect()\n",
    "\n",
    "# ---------- 6. Installments ----------\n",
    "with timer(\"Process Installments_payments data\"):\n",
    "    ins = read_csv_smart(DATA_DIR, \"installments_payments\")\n",
    "    \n",
    "    ins['DAYS_LATE'] = (ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']).clip(lower=0)\n",
    "    avg_late_days = ins.groupby('SK_ID_CURR')['DAYS_LATE'].mean().reset_index().rename(\n",
    "        columns={'DAYS_LATE': 'AVG_LATE_DAYS'})\n",
    "    ins['PAYMENT_RATIO'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    avg_payment_ratio = ins.groupby('SK_ID_CURR')['PAYMENT_RATIO'].mean().reset_index().rename(\n",
    "        columns={'PAYMENT_RATIO': 'AVG_PAYMENT_RATIO'})\n",
    "    \n",
    "    app_train = app_train.merge(avg_late_days,     on='SK_ID_CURR', how='left')\n",
    "    app_train = app_train.merge(avg_payment_ratio, on='SK_ID_CURR', how='left')\n",
    "    app_test  = app_test.merge(avg_late_days,      on='SK_ID_CURR', how='left')\n",
    "    app_test  = app_test.merge(avg_payment_ratio,  on='SK_ID_CURR', how='left')\n",
    "\n",
    "    del ins, avg_late_days, avg_payment_ratio\n",
    "    gc.collect()\n",
    "\n",
    "# ---------- 7. POS_CASH ----------\n",
    "with timer(\"Process POS_CASH_balance data\"):\n",
    "    pos = read_csv_smart(DATA_DIR, \"POS_CASH_balance\")\n",
    "    \n",
    "    avg_pos_dpd = pos.groupby('SK_ID_CURR')['SK_DPD'].mean().reset_index().rename(\n",
    "        columns={'SK_DPD': 'AVG_POS_DPD'})\n",
    "    max_pos_dpd = pos.groupby('SK_ID_CURR')['SK_DPD'].max().reset_index().rename(\n",
    "        columns={'SK_DPD': 'MAX_POS_DPD'})\n",
    "    pos_counts = pos.groupby('SK_ID_CURR').size().reset_index().rename(columns={0: 'POS_COUNT'})\n",
    "    \n",
    "    app_train = app_train.merge(avg_pos_dpd, on='SK_ID_CURR', how='left')\n",
    "    app_train = app_train.merge(max_pos_dpd, on='SK_ID_CURR', how='left')\n",
    "    app_train = app_train.merge(pos_counts,  on='SK_ID_CURR', how='left')\n",
    "    app_test  = app_test.merge(avg_pos_dpd, on='SK_ID_CURR', how='left')\n",
    "    app_test  = app_test.merge(max_pos_dpd, on='SK_ID_CURR', how='left')\n",
    "    app_test  = app_test.merge(pos_counts,  on='SK_ID_CURR', how='left')\n",
    "\n",
    "    del pos, avg_pos_dpd, max_pos_dpd, pos_counts\n",
    "    gc.collect()\n",
    "\n",
    "# ---------- 8. Credit Card ----------\n",
    "with timer(\"Process Credit_card_balance data\"):\n",
    "    cc = read_csv_smart(DATA_DIR, \"credit_card_balance\")\n",
    "    \n",
    "    cc_counts = cc.groupby('SK_ID_CURR')['SK_ID_PREV'].nunique().reset_index().rename(\n",
    "        columns={'SK_ID_PREV': 'CC_COUNT'})\n",
    "    avg_cc_balance = cc.groupby('SK_ID_CURR')['AMT_BALANCE'].mean().reset_index().rename(\n",
    "        columns={'AMT_BALANCE': 'AVG_CC_BALANCE'})\n",
    "    max_cc_dpd = cc.groupby('SK_ID_CURR')['SK_DPD'].max().reset_index().rename(\n",
    "        columns={'SK_DPD': 'MAX_CC_DPD'})\n",
    "    \n",
    "    app_train = app_train.merge(cc_counts,      on='SK_ID_CURR', how='left')\n",
    "    app_train = app_train.merge(avg_cc_balance, on='SK_ID_CURR', how='left')\n",
    "    app_train = app_train.merge(max_cc_dpd,     on='SK_ID_CURR', how='left')\n",
    "    app_test  = app_test.merge(cc_counts,       on='SK_ID_CURR', how='left')\n",
    "    app_test  = app_test.merge(avg_cc_balance,  on='SK_ID_CURR', how='left')\n",
    "    app_test  = app_test.merge(max_cc_dpd,      on='SK_ID_CURR', how='left')\n",
    "\n",
    "    del cc, cc_counts, avg_cc_balance, max_cc_dpd\n",
    "    gc.collect()\n",
    "\n",
    "# ---------- 9. 缺失值填充（修复 inf/-inf 问题） ----------\n",
    "with timer(\"Fill missing values\"):\n",
    "    # 先把 inf/-inf 统一替换为 NaN（比值分母为 0 时会产生 inf）\n",
    "    for df in (app_train, app_test):\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # 再做填充\n",
    "    app_train = app_train.fillna(-999)\n",
    "    app_test  = app_test.fillna(-999)\n",
    "    print('Training set shape:', app_train.shape)\n",
    "    print('Test set shape:', app_test.shape)\n",
    "\n",
    "# ------------------- 10. 训练 WOA-XGBoost（GPU, 使用 xgb.train） -------------------\n",
    "with timer(\"Train WOA-XGBoost (GPU)\"):\n",
    "    features = [c for c in app_train.columns if c not in ['TARGET', 'SK_ID_CURR']]\n",
    "    X, y = app_train[features], app_train['TARGET']\n",
    "    X_test = app_test[features]\n",
    "    \n",
    "    print('Number of features:', len(features))\n",
    "    print('X shape:', X.shape)\n",
    "    print('y shape:', y.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "\n",
    "    # ----- 参数空间 -----\n",
    "    space = {\n",
    "        \"max_depth\":          (3, 10, 'int'),\n",
    "        \"learning_rate\":      (0.01, 0.2, 'float'),\n",
    "        \"subsample\":          (0.5, 1.0, 'float'),\n",
    "        \"colsample_bytree\":   (0.5, 1.0, 'float'),\n",
    "        \"reg_alpha\":          (0.0, 10.0, 'float'),\n",
    "        \"reg_lambda\":         (0.0, 10.0, 'float'),\n",
    "        \"min_child_weight\":   (1.0, 50.0, 'float'),\n",
    "        \"gamma\":              (0.0, 10.0, 'float'),\n",
    "        \"n_estimators\":       (200, 2000, 'int')\n",
    "    }\n",
    "    keys = list(space.keys())\n",
    "    dim = len(keys)\n",
    "\n",
    "    def clip_cast(name, v):\n",
    "        low, high, tp = space[name]\n",
    "        v = np.minimum(np.maximum(v, low), high)\n",
    "        return int(round(v)) if tp == 'int' else float(v)\n",
    "\n",
    "    def vec_to_param_dict(vec):\n",
    "        # xgb.train 的 params（不包含 num_boost_round）\n",
    "        raw = {k: clip_cast(k, v) for k, v in zip(keys, vec)}\n",
    "        num_boost_round = int(raw.pop(\"n_estimators\"))\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"auc\",\n",
    "            \"tree_method\": \"gpu_hist\",\n",
    "            \"predictor\": \"gpu_predictor\",\n",
    "            \"max_depth\": raw[\"max_depth\"],\n",
    "            \"learning_rate\": raw[\"learning_rate\"],\n",
    "            \"subsample\": raw[\"subsample\"],\n",
    "            \"colsample_bytree\": raw[\"colsample_bytree\"],\n",
    "            \"reg_alpha\": raw[\"reg_alpha\"],\n",
    "            \"reg_lambda\": raw[\"reg_lambda\"],\n",
    "            \"min_child_weight\": raw[\"min_child_weight\"],\n",
    "            \"gamma\": raw[\"gamma\"],\n",
    "            \"max_bin\": 256,\n",
    "            \"verbosity\": 0,\n",
    "            \"seed\": RANDOM_STATE\n",
    "        }\n",
    "        return params, num_boost_round\n",
    "\n",
    "    # 预测工具：兼容不同版本的 best_ntree_limit / best_iteration\n",
    "    def predict_with_best(bst, dmat):\n",
    "        if hasattr(bst, \"best_ntree_limit\") and bst.best_ntree_limit:\n",
    "            return bst.predict(dmat, ntree_limit=bst.best_ntree_limit)\n",
    "        elif hasattr(bst, \"best_iteration\") and bst.best_iteration is not None:\n",
    "            try:\n",
    "                return bst.predict(dmat, iteration_range=(0, bst.best_iteration + 1))\n",
    "            except TypeError:\n",
    "                return bst.predict(dmat)\n",
    "        else:\n",
    "            return bst.predict(dmat)\n",
    "\n",
    "    # 适应度：3-Fold AUC + 早停（DMatrix 显式 missing=np.nan）\n",
    "    folds_woa = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    cache = {}\n",
    "\n",
    "    def fitness(vec):\n",
    "        key = tuple(np.round(vec, 6))\n",
    "        if key in cache:\n",
    "            return cache[key]\n",
    "        params, num_round = vec_to_param_dict(vec)\n",
    "        aucs = []\n",
    "        for trn_idx, val_idx in folds_woa.split(X, y):\n",
    "            X_trn, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "            y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "            dtrn = xgb.DMatrix(X_trn, label=y_trn, missing=np.nan)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val, missing=np.nan)\n",
    "            watchlist = [(dtrn, 'train'), (dval, 'valid')]\n",
    "\n",
    "            bst = xgb.train(\n",
    "                params=params,\n",
    "                dtrain=dtrn,\n",
    "                num_boost_round=num_round,\n",
    "                evals=watchlist,\n",
    "                early_stopping_rounds=200,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            pred_val = predict_with_best(bst, dval)\n",
    "            aucs.append(roc_auc_score(y_val, pred_val))\n",
    "            del bst, dtrn, dval, X_trn, X_val, y_trn, y_val\n",
    "        score = float(np.mean(aucs))\n",
    "        cache[key] = score\n",
    "        return score\n",
    "\n",
    "    # 初始化 WOA\n",
    "    pop_size = 12\n",
    "    max_iter = 15\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    lb = np.array([space[k][0] for k in keys], dtype=float)\n",
    "    ub = np.array([space[k][1] for k in keys], dtype=float)\n",
    "    population = lb + (ub - lb) * rng.rand(pop_size, dim)\n",
    "    fitness_vals = np.array([fitness(ind) for ind in population])\n",
    "    best_idx = int(np.argmax(fitness_vals))\n",
    "    best_pos = population[best_idx].copy()\n",
    "    best_score = float(fitness_vals[best_idx])\n",
    "    print(f\"WOA init best AUC: {best_score:.6f}\")\n",
    "\n",
    "    # 主循环\n",
    "    b = 1.0\n",
    "    for t in range(max_iter):\n",
    "        a = 2 - 2 * (t / (max_iter - 1 + 1e-12))\n",
    "        for i in range(pop_size):\n",
    "            r1, r2 = rng.rand(), rng.rand()\n",
    "            A = 2 * a * r1 - a\n",
    "            C = 2 * r2\n",
    "            p = rng.rand()\n",
    "            Xi = population[i].copy()\n",
    "\n",
    "            if p < 0.5:\n",
    "                if abs(A) < 1:\n",
    "                    D = np.abs(C * best_pos - Xi)\n",
    "                    new_pos = best_pos - A * D\n",
    "                else:\n",
    "                    rand_idx = rng.randint(pop_size)\n",
    "                    Xrand = population[rand_idx]\n",
    "                    D = np.abs(C * Xrand - Xi)\n",
    "                    new_pos = Xrand - A * D\n",
    "            else:\n",
    "                l = rng.uniform(-1, 1)\n",
    "                D = np.abs(best_pos - Xi)\n",
    "                new_pos = D * np.exp(b * l) * np.cos(2 * np.pi * l) + best_pos\n",
    "\n",
    "            new_pos = np.minimum(np.maximum(new_pos, lb), ub)\n",
    "            new_fit = fitness(new_pos)\n",
    "            if new_fit > fitness_vals[i]:\n",
    "                population[i] = new_pos\n",
    "                fitness_vals[i] = new_fit\n",
    "\n",
    "        iter_best_idx = int(np.argmax(fitness_vals))\n",
    "        iter_best_fit = float(fitness_vals[iter_best_idx])\n",
    "        if iter_best_fit > best_score:\n",
    "            best_score = iter_best_fit\n",
    "            best_pos = population[iter_best_idx].copy()\n",
    "        print(f\"WOA iter {t+1}/{max_iter} best AUC: {best_score:.6f}\")\n",
    "\n",
    "    # 最优参数并 5-Fold 训练\n",
    "    best_params, best_num_round = vec_to_param_dict(best_pos)\n",
    "    print(\"Best params from WOA:\")\n",
    "    for k in keys:\n",
    "        if k == \"n_estimators\":\n",
    "            continue\n",
    "        if k in best_params:\n",
    "            print(f\"  {k}: {best_params[k]}\")\n",
    "        else:\n",
    "            print(f\"  {k}: {clip_cast(k, dict(zip(keys, best_pos))[k])}\")\n",
    "    print(f\"  n_estimators (num_boost_round): {best_num_round}\")\n",
    "\n",
    "    n_folds = 5\n",
    "    folds = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    oof_preds = np.zeros(X.shape[0], dtype=float)\n",
    "    test_preds = np.zeros(X_test.shape[0], dtype=float)\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    dtest = xgb.DMatrix(X_test, missing=np.nan)\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y), 1):\n",
    "        print(f'Fold {fold_}')\n",
    "        X_trn, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "        y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "        dtrn = xgb.DMatrix(X_trn, label=y_trn, missing=np.nan)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val, missing=np.nan)\n",
    "\n",
    "        watchlist = [(dtrn, 'train'), (dval, 'valid')]\n",
    "        bst = xgb.train(\n",
    "            params=best_params,\n",
    "            dtrain=dtrn,\n",
    "            num_boost_round=best_num_round,\n",
    "            evals=watchlist,\n",
    "            early_stopping_rounds=200,\n",
    "            verbose_eval=200\n",
    "        )\n",
    "\n",
    "        oof_preds[val_idx] = predict_with_best(bst, dval)\n",
    "        test_preds += predict_with_best(bst, dtest) / n_folds\n",
    "\n",
    "        # 特征重要性（gain）\n",
    "        gain_map = bst.get_score(importance_type='gain')  # 可能是 f0/f1... 或列名\n",
    "        mapped = {}\n",
    "        for k, v in gain_map.items():\n",
    "            if k.startswith('f') and k[1:].isdigit():\n",
    "                idx = int(k[1:])\n",
    "                if 0 <= idx < len(features):\n",
    "                    mapped[features[idx]] = v\n",
    "            else:\n",
    "                mapped[k] = v\n",
    "        imp_series = pd.Series(mapped)\n",
    "        if not imp_series.empty:\n",
    "            fold_imp = pd.DataFrame({\n",
    "                \"feature\": imp_series.index,\n",
    "                \"importance\": imp_series.values,\n",
    "                \"fold\": fold_\n",
    "            })\n",
    "            feature_importance_df = pd.concat([feature_importance_df, fold_imp], axis=0)\n",
    "\n",
    "        del bst, dtrn, dval, X_trn, X_val, y_trn, y_val\n",
    "        gc.collect()\n",
    "\n",
    "    cv_auc = roc_auc_score(y, oof_preds)\n",
    "    print(f'Full AUC score: {cv_auc:.6f}')\n",
    "\n",
    "    # 特征重要性图（沿用文件名）\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    if feature_importance_df.empty:\n",
    "        print(\"Warning: feature_importance_df is empty; using zeros as fallback.\")\n",
    "        importance = pd.Series(0, index=features, dtype=float)\n",
    "    else:\n",
    "        feature_importance = (feature_importance_df\n",
    "                              .groupby('feature')['importance']\n",
    "                              .mean()\n",
    "                              .sort_values(ascending=False))\n",
    "        importance = feature_importance\n",
    "\n",
    "    top_features = importance.head(30).index\n",
    "    sns.barplot(y=top_features, x=importance[top_features], orient='h')\n",
    "    plt.title('XGBoost Features (Top 30 by gain importance)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_feature_importance.png')\n",
    "    plt.close()\n",
    "\n",
    "    # ROC 曲线\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    fpr, tpr, _ = roc_curve(y, oof_preds)\n",
    "    plt.plot(fpr, tpr, label=f'CV AUC: {cv_auc:.4f}')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('roc_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "# ---------- 11. 生成提交文件 ----------\n",
    "with timer(\"Generate prediction results\"):\n",
    "    submission = pd.DataFrame({\n",
    "        'SK_ID_CURR': app_test['SK_ID_CURR'],\n",
    "        'TARGET': test_preds\n",
    "    })\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print('Prediction results have been saved as submission.csv')\n",
    "\n",
    "print('Done! Final CV score:', cv_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d32981f-63d9-4327-8aad-3ae5ae07547a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
